# config.yaml
input_dim: 2048
seq_len: 512
mlp_hidden_dim: 4096
num_transformer_layers: 2
num_attention_heads: 8
dropout: 0.1
lr: 5e-5
batch_size: 16
num_epochs: 10
gradient_accumulation_steps: 2
max_grad_norm: 1.0
sample_every_n_steps: 1000
num_workers: 16
json_data_path: "data/data.json"
sdxl_model_path: "stabilityai/stable-diffusion-xl-base-1.0"
llama_model_path: "meta-llama/Llama-3.2-1B"
output_dir: "output"
# adapter_checkpoint: "output/final_adapter.pth"
use_cross_attn: true

example_prompt: "A photo of a cat wearing a hat"